{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Counter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnlp_utils\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mu\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCounter\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword_tokenize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mword_tokenize\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Counter'"
     ]
    }
   ],
   "source": [
    "\n",
    "import nlp_utils as u\n",
    "import Counter\n",
    "import word_tokenize as wt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m lower_tokens \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create a Counter with the lowercase tokens: bow_simple\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m bow_simple \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m(lower_tokens)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Print the 10 most common tokens\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(bow_simple\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "article = u.get_sample_article()\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = article.split()\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [t.lower() for t in tokens]\n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_simple = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_simple.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(29, 0.003947336371077938), (34, 0.015722570834963884), (36, 0.012459842594174939), (40, 0.015722570834963884), (49, 0.06289028333985554)]\n",
      "reverse 0.4245094125440249\n",
      "engineering 0.347517588472571\n",
      "3d 0.22545930743311063\n",
      "product 0.14224789608159819\n",
      "chikofsky 0.14091206714569413\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nlp_utils import get_pre_process_wiki_articles\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "articles = get_pre_process_wiki_articles()\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Get the fifth document in corpus: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Create a new TfidfModel using the corpus: tfidf\n",
    "tfidf = TfidfModel(corpus)\n",
    "# Calculate the tfidf weights of doc: tfidf_weights\n",
    "tfidf_weights = tfidf[doc]\n",
    "\n",
    "# Print the first five weights\n",
    "print(tfidf_weights[:5])\n",
    "\n",
    "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
    "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 weighted words\n",
    "for term_id, weight in sorted_tfidf_weights[:5]:\n",
    "    print(dictionary.get(term_id), weight)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9b9b9411a65a0f6e5c1dc01ef6b610efec134e693b38e7a5717e6d7272134454"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
